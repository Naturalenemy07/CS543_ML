{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tika import parser\n",
    "from nltk.corpus import stopwords\n",
    "import re   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    review_text = parser.from_buffer(\"<html>\" + raw_review + \"</html>\")[\"content\"]\n",
    "    # print(review_text)\n",
    " \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # print(letters_only)\n",
    "    words = letters_only.lower().split()\n",
    "    # print(words)\n",
    "    # print(len(words))\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "['id' 'sentiment' 'review']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5236/3201425762.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(range(0, num_reviews)):                                  #5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36af283316d64677a62acfc7352ca26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    " \n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)                           #1\n",
    "print(train.shape)                                                       #2\n",
    "print(train.columns.values)                                              #2\n",
    " \n",
    "num_reviews = train[\"review\"].size                                       #3\n",
    "clean_train_reviews = []                                                 #4\n",
    " \n",
    "for i in tqdm(range(0, num_reviews)):                                  #5\n",
    "    clean_train_reviews.append(review_to_words(train[\"review\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \n",
    " \n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "train_data_features = train_data_features.toarray()\n",
    "print(train_data_features.shape)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(vocab), vocab))\n\u001b[1;32m      4\u001b[0m dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(train_data_features, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(\"size %d %s \" % (len(vocab), vocab))\n",
    " \n",
    "dist = np.sum(train_data_features, axis=0)\n",
    " \n",
    "for tag, count in zip(vocab, dist):\n",
    "    print(\"%d, %s\" % (count, tag))\n",
    " \n",
    "plt.scatter(vocab[0:99], dist[0:99])           #4\n",
    "plt.xticks(vocab[0:99], rotation='vertical')   #4\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() # A class that saves and restores  variables  from checkpoints\n",
    "with tf.Session() as sess: # A session is a class that owns resources on computer, needs to run \"with\" so that session closes and resources are freed up\n",
    "    sess.run(tf.global_variables_initializer()) # runs operations and evaluates tensors\n",
    "    prev_err = 0.     #0. is a float                                                   \n",
    "    for epoch in tqdm(range(training_epochs)): # creates the progress bar\n",
    "        feed_dict = {} # initializes dictionary to store \n",
    "        for i in range(train_data_features.shape[1]): \n",
    "            feed_dict[Xs[\"X\"+str(i)]] = train_data_features[:,i,None].reshape(len(train_data_features)) # puts stuff from train_data_features into new dictionary\n",
    "        feed_dict[Y] = ys # set Y in feed dict to the train sentiment values                                                #3\n",
    "        err, _ = sess.run([cost, train_op], feed_dict=feed_dict) # trains or something, outputs an error\n",
    "        print(epoch, err)\n",
    "        if abs(prev_err - err) < 0.0001:    # if difference between error and previous error is less than 0.0001, break                          #4\n",
    "            break                                                         #4\n",
    "        prev_err = err # otherwise, set prev_err to err\n",
    "    \n",
    "    w_val = sess.run(w, feed_dict)   # w_val inputting logistic regression weights\n",
    "    save_path = saver.save(sess, \"./en-netflix-binary-sentiment.ckpt\")    # saves the model\n",
    " \n",
    "print(w_val) # prints the weights\n",
    "print(np.max(w_val)) # prints the max weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
